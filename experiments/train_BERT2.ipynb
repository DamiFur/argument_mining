{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importamos librerias necesarias y definimos clases para sentencias y ejemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 12:56:22.262555 140628687111936 file_utils.py:39] PyTorch version 1.2.0 available.\n",
      "I1113 12:56:22.306426 140628687111936 modeling_xlnet.py:194] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from transformers import AdamW, WarmupLinearSchedule\n",
    "from transformers import WEIGHTS_NAME, BertConfig, BertForTokenClassification, BertTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "from io import open\n",
    "\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CLASSES = {\n",
    "    \"bert\": (BertConfig, BertForTokenClassification, BertTokenizer),\n",
    "}\n",
    "\n",
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for token classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, words, labels):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "        Args:\n",
    "            guid: Unique id for the example.\n",
    "            words: list. The words of the sequence.\n",
    "            labels: (Optional) list. The labels for each word of the sequence. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.words = words\n",
    "        self.labels = labels\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_ids):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_ids = label_ids\n",
    "\n",
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, t) for w, t in zip(s[\"text\"].values.tolist(),\n",
    "                                                           s[\"tag\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"sentence\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = \"cuda\"\n",
    "n_gpu = 1\n",
    "\n",
    "# Set seed\n",
    "set_seed(42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "with torch.cuda.device(n_gpu):\n",
    "    print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bajamos el modelo, config y tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 12:56:28.420320 140628687111936 configuration_utils.py:151] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-config.json from cache at /home/dfurman/.cache/torch/transformers/6dfaed860471b03ab5b9acb6153bea82b6632fb9bbe514d3fff050fe1319ee6d.4c88e2dec8f8b017f319f6db2b157fee632c0860d9422e4851bd0d6999f9ce38\n",
      "I1113 12:56:28.421726 140628687111936 configuration_utils.py:168] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I1113 12:56:29.055879 140628687111936 tokenization_utils.py:374] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-vocab.txt from cache at /home/dfurman/.cache/torch/transformers/9b3c03a36e83b13d5ba95ac965c9f9074a99e14340c523ab405703179e79fc46.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I1113 12:56:29.751401 140628687111936 modeling_utils.py:337] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-pytorch_model.bin from cache at /home/dfurman/.cache/torch/transformers/54da47087cc86ce75324e4dc9bbb5f66c6e83a7c6bd23baea8b489acc8d09aa4.4d5343a4b979c4beeaadef17a0453d1bb183dd9b084f58b84c7cc781df343ae6\n",
      "I1113 12:56:39.295124 140628687111936 modeling_utils.py:405] Weights of BertForTokenClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "I1113 12:56:39.295886 140628687111936 modeling_utils.py:408] Weights from pretrained model not used in BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n"
     ]
    }
   ],
   "source": [
    "with torch.cuda.device(n_gpu):\n",
    "    # Use cross entropy ignore index as padding label id so that only real label ids contribute to the loss later\n",
    "    pad_token_label_id = CrossEntropyLoss().ignore_index\n",
    "\n",
    "    model_type = \"bert\"\n",
    "    config_class, model_class, tokenizer_class = MODEL_CLASSES[model_type]\n",
    "\n",
    "    bert_type = \"bert-large-uncased\"\n",
    "    config = config_class.from_pretrained(bert_type,\n",
    "                                              num_labels=2)\n",
    "    tokenizer = tokenizer_class.from_pretrained(bert_type,\n",
    "                                                    do_lower_case=True)\n",
    "    model = model_class.from_pretrained(bert_type, from_tf=False,\n",
    "                                            config=config)\n",
    "    model.to(device)\n",
    "\n",
    "    'done'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funcion auxiliar para convertir a features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples,\n",
    "                                 label_list,\n",
    "                                 max_seq_length,\n",
    "                                 tokenizer,\n",
    "                                 cls_token_at_end=False,\n",
    "                                 cls_token=\"[CLS]\",\n",
    "                                 cls_token_segment_id=1,\n",
    "                                 sep_token=\"[SEP]\",\n",
    "                                 sep_token_extra=False,\n",
    "                                 pad_on_left=False,\n",
    "                                 pad_token=0,\n",
    "                                 pad_token_segment_id=0,\n",
    "                                 pad_token_label_id=-1,\n",
    "                                 sequence_a_segment_id=0,\n",
    "                                 mask_padding_with_zero=True):\n",
    "    \"\"\" Loads a data file into a list of `InputBatch`s\n",
    "        `cls_token_at_end` define the location of the CLS token:\n",
    "            - False (Default, BERT/XLM pattern): [CLS] + A + [SEP] + B + [SEP]\n",
    "            - True (XLNet/GPT pattern): A + [SEP] + B + [SEP] + [CLS]\n",
    "        `cls_token_segment_id` define the segment id associated to the CLS token (0 for BERT, 2 for XLNet)\n",
    "    \"\"\"\n",
    "\n",
    "    label_map = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "#         if ex_index % 10000 == 0:\n",
    "#             print(\"Writing example {} of {}\".format(ex_index, len(examples)))\n",
    "#             print(\"E.g: {}\".format(example.words))\n",
    "\n",
    "        tokens = []\n",
    "        label_ids = []\n",
    "        for word, label in zip(example.words, example.labels):\n",
    "            word_tokens = tokenizer.tokenize(word)\n",
    "            tokens.extend(word_tokens)\n",
    "            # Use the real label id for the first token of the word, and padding ids for the remaining tokens\n",
    "            label_ids.extend([label_map[label]] + [pad_token_label_id] * (len(word_tokens) - 1))\n",
    "\n",
    "        # Account for [CLS] and [SEP] with \"- 2\" and with \"- 3\" for RoBERTa.\n",
    "        special_tokens_count = 3 if sep_token_extra else 2\n",
    "        if len(tokens) > max_seq_length - special_tokens_count:\n",
    "            tokens = tokens[:(max_seq_length - special_tokens_count)]\n",
    "            label_ids = label_ids[:(max_seq_length - special_tokens_count)]\n",
    "\n",
    "        # The convention in BERT is:\n",
    "        # (a) For sequence pairs:\n",
    "        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "        #  type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1\n",
    "        # (b) For single sequences:\n",
    "        #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "        #  type_ids:   0   0   0   0  0     0   0\n",
    "        #\n",
    "        # Where \"type_ids\" are used to indicate whether this is the first\n",
    "        # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "        # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "        # embedding vector (and position vector). This is not *strictly* necessary\n",
    "        # since the [SEP] token unambiguously separates the sequences, but it makes\n",
    "        # it easier for the model to learn the concept of sequences.\n",
    "        #\n",
    "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
    "        # the entire model is fine-tuned.\n",
    "        tokens += [sep_token]\n",
    "        label_ids += [pad_token_label_id]\n",
    "        if sep_token_extra:\n",
    "            # roberta uses an extra separator b/w pairs of sentences\n",
    "            tokens += [sep_token]\n",
    "            label_ids += [pad_token_label_id]\n",
    "        segment_ids = [sequence_a_segment_id] * len(tokens)\n",
    "\n",
    "        if cls_token_at_end:\n",
    "            tokens += [cls_token]\n",
    "            label_ids += [pad_token_label_id]\n",
    "            segment_ids += [cls_token_segment_id]\n",
    "        else:\n",
    "            tokens = [cls_token] + tokens\n",
    "            label_ids = [pad_token_label_id] + label_ids\n",
    "            segment_ids = [cls_token_segment_id] + segment_ids\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding_length = max_seq_length - len(input_ids)\n",
    "        if pad_on_left:\n",
    "            input_ids = ([pad_token] * padding_length) + input_ids\n",
    "            input_mask = ([0 if mask_padding_with_zero else 1] * padding_length) + input_mask\n",
    "            segment_ids = ([pad_token_segment_id] * padding_length) + segment_ids\n",
    "            label_ids = ([pad_token_label_id] * padding_length) + label_ids\n",
    "        else:\n",
    "            input_ids += ([pad_token] * padding_length)\n",
    "            input_mask += ([0 if mask_padding_with_zero else 1] * padding_length)\n",
    "            segment_ids += ([pad_token_segment_id] * padding_length)\n",
    "            label_ids += ([pad_token_label_id] * padding_length)\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "        assert len(label_ids) == max_seq_length\n",
    "\n",
    "        features.append(\n",
    "                InputFeatures(input_ids=input_ids,\n",
    "                              input_mask=input_mask,\n",
    "                              segment_ids=segment_ids,\n",
    "                              label_ids=label_ids))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_dataloader):\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    preds = None\n",
    "    out_label_ids = None\n",
    "    model.eval()\n",
    "    device = \"cuda\"\n",
    "    with torch.cuda.device(1):\n",
    "        for batch in test_dataloader:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            with torch.no_grad():\n",
    "                inputs = {\"input_ids\": batch[0],\n",
    "                          \"attention_mask\": batch[1],\n",
    "                          \"token_type_ids\": batch[2],\n",
    "                          # XLM and RoBERTa don\"t use segment_ids\n",
    "                          \"labels\": batch[3]}\n",
    "                outputs = model(**inputs)\n",
    "                tmp_eval_loss, logits = outputs[:2]\n",
    "\n",
    "\n",
    "                eval_loss += tmp_eval_loss.item()\n",
    "            nb_eval_steps += 1\n",
    "            if preds is None:\n",
    "                preds = logits.detach().cpu().numpy()\n",
    "                out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n",
    "            else:\n",
    "                preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "                out_label_ids = np.append(out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0)\n",
    "        eval_loss = eval_loss / nb_eval_steps\n",
    "        preds = np.argmax(preds, axis=2)\n",
    "\n",
    "        return eval_loss, preds, out_label_ids\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataset, model, tokenizer, labels, pad_token_label_id, dev_dataloader):\n",
    "    \"\"\" Train the model \"\"\"\n",
    "    \n",
    "    train_batch_size = 4\n",
    "    train_sampler = RandomSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=train_batch_size)\n",
    "\n",
    "    num_train_epochs = 2.0\n",
    "    t_total = len(train_dataloader) // num_train_epochs\n",
    "\n",
    "    # Prepare optimizer and schedule (linear warmup and decay)\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "         \"weight_decay\": 0.1},\n",
    "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0}\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=5e-5, eps=1e-8)\n",
    "    scheduler = WarmupLinearSchedule(optimizer, warmup_steps=0, t_total=t_total)\n",
    "\n",
    "\n",
    "    global_step = 0\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "    train_loss_timeline = []\n",
    "    eval_loss_timeline = []\n",
    "    f1_timeline = []\n",
    "    model.zero_grad()\n",
    "    train_iterator = trange(int(num_train_epochs), desc=\"Epoch\", disable=True)\n",
    "    set_seed(42)  # Added here for reproductibility (even between python 2 and 3)\n",
    "    \n",
    "    for _ in train_iterator:\n",
    "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=False)\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "            model.train()\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            inputs = {\"input_ids\": batch[0],\n",
    "                        \"attention_mask\": batch[1],\n",
    "                        \"token_type_ids\": batch[2],\n",
    "                        # XLM and RoBERTa don\"t use segment_ids\n",
    "                        \"labels\": batch[3]\n",
    "            }\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs[0]  # model outputs are always tuple in pytorch-transformers (see doc)\n",
    "            loss.backward()\n",
    "            tr_loss += loss.item()\n",
    "            optimizer.step()\n",
    "            scheduler.step()  # Update learning rate schedule\n",
    "            model.zero_grad()\n",
    "            \n",
    "            global_step += 1\n",
    "\n",
    "        # Evaluamos\n",
    "        loss, _, _ = evaluate(model, dev_dataloader)\n",
    "        eval_loss_timeline.append(loss)\n",
    "        train_loss_timeline.append(tr_loss / global_step)\n",
    "    tr_loss = 0\n",
    "    global_step = 0\n",
    "            \n",
    "\n",
    "    return global_step, train_loss_timeline, eval_loss_timeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Motion', 'is', 'one', 'of', 'the', 'most', 'important', 'ingredients', 'of', 'CG', 'movies', 'and', 'computer', 'games', '.']\n",
      "['claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'O']\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"../train_mm.txt\", sep='\\t', encoding=\"latin1\").fillna(method=\"ffill\")\n",
    "\n",
    "getter = SentenceGetter(data)\n",
    "#Sacarle las oraciones que empiezan con \"<\", el autor, abstract, etc\n",
    "\n",
    "sentences = [[s[0] for s in sent] for sent in getter.sentences]\n",
    "labels = [[s[1] for s in sent] for sent in getter.sentences]\n",
    "\n",
    "num_labels = len(labels)\n",
    "\n",
    "examples = [InputExample(guid, words, labels) for guid, (words, labels) in enumerate(zip(sentences, labels))]\n",
    "print(examples[22].words)\n",
    "print(examples[22].labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.cuda.device(n_gpu):\n",
    "    possible_labels = [\"O\", \"claim\"]\n",
    "    features = convert_examples_to_features(examples, possible_labels, 50, tokenizer,\n",
    "                                                    cls_token_at_end=False,\n",
    "                                                    # xlnet has a cls token at the end\n",
    "                                                    cls_token=tokenizer.cls_token,\n",
    "                                                    cls_token_segment_id=0,\n",
    "                                                    sep_token=tokenizer.sep_token,\n",
    "                                                    sep_token_extra=False,\n",
    "                                                    # roberta uses an extra separator b/w pairs of sentences, cf. github.com/pytorch/fairseq/commit/1684e166e3da03f5b600dbb7855cb98ddfcd0805\n",
    "                                                    pad_on_left=False,\n",
    "                                                    # pad on the left for xlnet\n",
    "                                                    pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n",
    "                                                    pad_token_segment_id=0,\n",
    "                                                    pad_token_label_id=pad_token_label_id\n",
    "                                                    )\n",
    "\n",
    "\n",
    "        # Convert to Tensors and build dataset\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "    all_label_ids = torch.tensor([f.label_ids for f in features], dtype=torch.long)\n",
    "\n",
    "    all_input_ids.to(device)\n",
    "    all_input_mask.to(device)\n",
    "    all_segment_ids.to(device)\n",
    "    all_label_ids.to(device)\n",
    "\n",
    "    train_dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2554"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "devdata = pd.read_csv(\"../dev_mm.txt\", sep='\\t', encoding=\"latin1\").fillna(method=\"ffill\")\n",
    "\n",
    "getter = SentenceGetter(devdata)\n",
    "\n",
    "sentences_dev = [[s[0] for s in sent] for sent in getter.sentences]\n",
    "labels_dev = [[s[1] for s in sent] for sent in getter.sentences]\n",
    "\n",
    "num_labels = len(labels_dev)\n",
    "\n",
    "examples_dev = [InputExample(guid, words, labels) for guid, (words, labels) in enumerate(zip(sentences_dev, labels_dev))]\n",
    "\n",
    "len(examples_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.cuda.device(1):\n",
    "    device = \"cuda\"\n",
    "    possible_labels = [\"O\", \"claim\"]\n",
    "    features_dev = convert_examples_to_features(examples_dev, possible_labels, 50, tokenizer,\n",
    "                                                    cls_token_at_end=False,\n",
    "                                                    # xlnet has a cls token at the end\n",
    "                                                    cls_token=tokenizer.cls_token,\n",
    "                                                    cls_token_segment_id=0,\n",
    "                                                    sep_token=tokenizer.sep_token,\n",
    "                                                    sep_token_extra=False,\n",
    "                                                    # roberta uses an extra separator b/w pairs of sentences, cf. github.com/pytorch/fairseq/commit/1684e166e3da03f5b600dbb7855cb98ddfcd0805\n",
    "                                                    pad_on_left=False,\n",
    "                                                    # pad on the left for xlnet\n",
    "                                                    pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n",
    "                                                    pad_token_segment_id=0,\n",
    "                                                    pad_token_label_id=pad_token_label_id\n",
    "                                                    )\n",
    "\n",
    "        # Convert to Tensors and build dataset\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features_dev], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in features_dev], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in features_dev], dtype=torch.long)\n",
    "    all_label_ids = torch.tensor([f.label_ids for f in features_dev], dtype=torch.long)\n",
    "    \n",
    "    all_input_ids.to(device)\n",
    "    all_input_mask.to(device)\n",
    "    all_segment_ids.to(device)\n",
    "    all_label_ids.to(device)\n",
    "\n",
    "    dev_dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "    dev_sampler = SequentialSampler(dev_dataset)\n",
    "    dev_dataloader = DataLoader(dev_dataset, sampler=dev_sampler, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_label_ids[22]\n",
    "# loss, preds, truth = evaluate(model, dev_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43056195301704286"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# truth_list = [(o,p) for sentence in zip(truth, preds) for (o,p) in zip(sentence[0], sentence[1]) if o != -100]\n",
    "\n",
    "# t, p = [i for (i,j) in truth_list], [j for (i,j) in truth_list]\n",
    "\n",
    "# f1score(t, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corremos el entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 4153/4153 [12:19<00:00,  5.63it/s]\n",
      "Iteration: 100%|██████████| 4153/4153 [12:20<00:00,  5.78it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tr_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-10505af3e235>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_gpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss_timeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_loss_timeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_token_label_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" global_step = %s, average loss = %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tr_loss' is not defined"
     ]
    }
   ],
   "source": [
    "#train_dataset = load_and_cache_examples(tokenizer, labels, pad_token_label_id, mode=\"train\")\n",
    "with torch.cuda.device(n_gpu):\n",
    "    global_step, train_loss_timeline, dev_loss_timeline = train(train_dataset, model, tokenizer, labels, pad_token_label_id, dev_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6023154196329414, 0.6023154196329414]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_loss_timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe5e2c60dd8>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAFi1JREFUeJzt3X+s3Xd93/Hn694bZ1CgAXyzQeyQTFyrTWnHulOPNYIlsKQGVUn+QGkifgSEksGUSR1bRKJ1apWu0hitmCpZa0MIv9QsQLbGHhSsloU1ijDy9RpCbGRiHNpczGZjHIQUEf9674/zcXJycq7P19f3R679fEhH93y+3/f3+/18fK/P634+53vsVBWSJE2sdAckSS8OBoIkCTAQJEmNgSBJAgwESVJjIEiSgI6BkGRTkj1J9ia5fZ6a65PsTrIryb1t2+uS7EzySNv+wYH6r7dzPtIeFy7OkCRJC5Fxn0NIMgl8F7gKmAN2ADdW1e6BmhngC8Bbq+pwkgur6kCSNe0azyR5GfAY8OtVtT/J14F/V1WzSzIySdJp6TJD2Ajsrap9VXUEuA+4dqjmZmBzVR0GqKoD7euRqnqm1Zzf8XqSpBUw1aHmIuDJgfYc8E+HajYAJHkYmAR+r6q+2ratB74MvB64rar2Dxz3qSTHgf8O/McaM11Zu3ZtXXLJJR26LEk6aefOnT+qqulxdV0CISO2Db9wTwEzwBXAOuChJG+oqqeq6kngV5K8Fnggyf1V9f+Ad1XVD5K8nH4gvAf47AsuntwC3AJw8cUXMzvrCpMknY4kf9ulrssSzhywfqC9Dtg/omZLVR2tqieAPfQD4lltZrALeHNr/6B9/SlwL/2lqReoqruqqldVvenpsQEnSVqgLoGwA5hJcml7k/gGYOtQzQPAlQBJ1tJfQtqXZF2Sl7TtrwQuB/YkmWp1JDkP+E36bzhLklbI2CWjqjqW5FZgG/33B+6pql1J7gRmq2pr23d1kt3AcfrvFRxKchXwR0mK/tLTH1bVt5P8HLCthcEk8FfAJ5ZkhJKkTsbedvpi0uv1yvcQJOn0JNlZVb1xdd4GKkkCDARJUmMgSJKAbp9DWPX+x/+Z4/uHnua8iTA5GaYmwtTEBFOTYXJiVHuCqXlqz5uY6NcMtlvdcPvktmTURzkk6cXlnAiE//mt/Ty45+CKXX/y2bBogTM58VxgTA4EUAua02sPPB9qnzcZJidGtwfD77l9o9tTA4H3gvbQ9Q1AafU6JwLhU+/fSFVx/ERxrD2OHy+OnTgxun38ZO3z20dPnGh1A/sH981Te/I8g+3n1xbH27me7U9r/+zoCY6dOP68/YPXPtk+evzEs+M73h4r5bkAmnheUAy3Xxhe8wfSc8dNDOx7YXtUQJ5qX5fwHAzi4b5NBANQZ41zIhCg/5d2ajJMTa50T5bHiRPF8cEQPH5i3vAYbB87PhBYz4bWiYF9HdunEV6D7aePHJt3X7/9/OudPP8K5t/Y2dToZcnR7ectU867bNnC6nnHhsnJiYFZaAvLU7SnBpY/n73+8Cx0qD8TE4bf2eycCYRzzcREmCCcdw4F4HwBNC6QBsPydMPzZDiNCqtx4Xn8RPHMseOnnJWOml0ePb5y6ZcwIrCGZ1LzL0U+F5innumNmvnNN9s81czvecE3pj1qlnquzf4MBJ0VJibCmmd/ez37U/BkAA4GxvCy5bF5Zm7jlimfW0YdWFIdmD2ObA9ef2gJ9mSQHjtxgp8de2F4Dp7ruX3Pb6+UiTB22fJUy5Tjw2t0WE4Ntc+bDNf8o9dywUvXLOl4DQRpFToZgGvOgTvHq4oTxbwzv1Ezu3HtU80Sh8PqVO1jI8LrZPvIsRdev2tfR7n89WsNBEnntiRMBiYnJjn/HHjFGrwBZnD29PMvOW/Jr30O/PFK0uqxkjfAnP3zTUlSJwaCJAkwECRJjYEgSQIMBElSYyBIkgADQZLUGAiSJMBAkCQ1BoIkCTAQJElNp0BIsinJniR7k9w+T831SXYn2ZXk3rbtdUl2Jnmkbf/gQP0/SfLtds4/zrn2D49L0ovM2H/cLskksBm4CpgDdiTZWlW7B2pmgDuAy6vqcJIL264fAr9eVc8keRnwWDt2P/BfgVuA7cBfAJuAryzi2CRJp6HLDGEjsLeq9lXVEeA+4NqhmpuBzVV1GKCqDrSvR6rqmVZz/snrJXkN8Iqq+kZVFfBZ4LozHo0kacG6BMJFwJMD7bm2bdAGYEOSh5NsT7Lp5I4k65M82s7x0TY7uKid51TnPHn8LUlmk8wePHiwQ3clSQvRJRBGre0P/5c+U8AMcAVwI3B3kgsAqurJqvoV4PXATUn+fsdz0o6/q6p6VdWbnp7u0F1J0kJ0CYQ5YP1Aex2wf0TNlqo6WlVPAHvoB8Sz2sxgF/DmVr9uzDklScuoSyDsAGaSXJpkDXADsHWo5gHgSoAka+kvIe1Lsi7JS9r2VwKXA3uq6ofAT5O8qd1d9F5gy6KMSJK0IGMDoaqOAbcC24DvAF+oql1J7kxyTSvbBhxKsht4ELitqg4Bvwh8M8m3gP8N/GFVfbsd8yHgbmAv8D28w0iSVlT6N/msDr1er2ZnZ1e6G5K0qiTZWVW9cXV+UlmSBBgIkqTGQJAkAQaCJKkxECRJgIEgSWoMBEkSYCBIkhoDQZIEGAiSpMZAkCQBBoIkqTEQJEmAgSBJagwESRJgIEiSGgNBkgQYCJKkxkCQJAEGgiSpMRAkSYCBIElqDARJEtAxEJJsSrInyd4kt89Tc32S3Ul2Jbm3bXtjkm+0bY8m+a2B+k8neSLJI+3xxsUZkiRpIabGFSSZBDYDVwFzwI4kW6tq90DNDHAHcHlVHU5yYdv1NPDeqno8yWuBnUm2VdVTbf9tVXX/Yg5IkrQwXWYIG4G9VbWvqo4A9wHXDtXcDGyuqsMAVXWgff1uVT3enu8HDgDTi9V5SdLi6RIIFwFPDrTn2rZBG4ANSR5Osj3JpuGTJNkIrAG+N7D5D9pS0seTnD/q4kluSTKbZPbgwYMduitJWogugZAR22qoPQXMAFcANwJ3J7ng2RMkrwE+B7y/qk60zXcAvwD8GvAq4COjLl5Vd1VVr6p609NOLiRpqXQJhDlg/UB7HbB/RM2WqjpaVU8Ae+gHBEleAXwZ+J2q2n7ygKr6YfU9A3yK/tKUJGmFdAmEHcBMkkuTrAFuALYO1TwAXAmQZC39JaR9rf7Pgc9W1RcHD2izBpIEuA547EwGIkk6M2PvMqqqY0luBbYBk8A9VbUryZ3AbFVtbfuuTrIbOE7/7qFDSd4NvAV4dZL3tVO+r6oeAf4syTT9JalHgA8u9uAkSd2lavjtgBevXq9Xs7OzK90NSVpVkuysqt64Oj+pLEkCDARJUmMgSJIAA0GS1BgIkiTAQJAkNQaCJAkwECRJjYEgSQIMBElSYyBIkgADQZLUGAiSJMBAkCQ1BoIkCTAQJEmNgSBJAgwESVJjIEiSAANBktQYCJIkwECQJDWdAiHJpiR7kuxNcvs8Ndcn2Z1kV5J727Y3JvlG2/Zokt8aqL80yTeTPJ7k80nWLM6QJEkLMTYQkkwCm4G3A5cBNya5bKhmBrgDuLyqfgn47bbraeC9bdsm4L8kuaDt+yjw8aqaAQ4DH1iE8UiSFqjLDGEjsLeq9lXVEeA+4NqhmpuBzVV1GKCqDrSv362qx9vz/cABYDpJgLcC97fjPwNcd6aDkSQtXJdAuAh4cqA917YN2gBsSPJwku1JNg2fJMlGYA3wPeDVwFNVdewU55QkLaOpDjUZsa1GnGcGuAJYBzyU5A1V9RRAktcAnwNuqqoTbYYw7py0Y28BbgG4+OKLO3RXkrQQXWYIc8D6gfY6YP+Imi1VdbSqngD20A8IkrwC+DLwO1W1vdX/CLggydQpzglAVd1VVb2q6k1PT3cZkyRpAboEwg5gpt0VtAa4Adg6VPMAcCVAkrX0l5D2tfo/Bz5bVV88WVxVBTwIvLNtugnYciYDkSSdmbGB0Nb5bwW2Ad8BvlBVu5LcmeSaVrYNOJRkN/0X+tuq6hBwPfAW4H1JHmmPN7ZjPgJ8OMle+u8pfHJRRyZJOi3p/7K+OvR6vZqdnV3pbkjSqpJkZ1X1xtX5SWVJEmAgSJIaA0GSBBgIkqTGQJAkAQaCJKkxECRJgIEgSWoMBEkSYCBIkhoDQZIEGAiSpMZAkCQBBoIkqTEQJEmAgSBJagwESRJgIEiSGgNBkgQYCJKkxkCQJAEGgiSpMRAkSUDHQEiyKcmeJHuT3D5PzfVJdifZleTege1fTfJUki8N1X86yRNJHmmPN57ZUCRJZ2JqXEGSSWAzcBUwB+xIsrWqdg/UzAB3AJdX1eEkFw6c4mPAS4F/OeL0t1XV/WcyAEnS4ugyQ9gI7K2qfVV1BLgPuHao5mZgc1UdBqiqAyd3VNXXgJ8uUn8lSUukSyBcBDw50J5r2wZtADYkeTjJ9iSbOl7/D5I8muTjSc7veIwkaQl0CYSM2FZD7SlgBrgCuBG4O8kFY857B/ALwK8BrwI+MvLiyS1JZpPMHjx4sEN3JUkL0SUQ5oD1A+11wP4RNVuq6mhVPQHsoR8Q86qqH1bfM8Cn6C9Njaq7q6p6VdWbnp7u0F1J0kJ0CYQdwEySS5OsAW4Atg7VPABcCZBkLf0lpH2nOmmS17SvAa4DHju9rkuSFtPYu4yq6liSW4FtwCRwT1XtSnInMFtVW9u+q5PsBo7Tv3voEECSh+gvDb0syRzwgaraBvxZkmn6S1KPAB9cgvFJkjpK1fDbAS9evV6vZmdnV7obkrSqJNlZVb1xdX5SWZIEGAiSpMZAkCQBBoIkqTEQJEmAgSBJagwESRJgIEiSGgNBkgQYCJKkxkCQJAEGgiSpMRAkSYCBIElqDARJEmAgSJIaA0GSBBgIkqTGQJAkAQaCJKkxECRJgIEgSWoMBEkS0DEQkmxKsifJ3iS3z1NzfZLdSXYluXdg+1eTPJXkS0P1lyb5ZpLHk3w+yZozG4ok6UyMDYQkk8Bm4O3AZcCNSS4bqpkB7gAur6pfAn57YPfHgPeMOPVHgY9X1QxwGPjAgkYgSVoUXWYIG4G9VbWvqo4A9wHXDtXcDGyuqsMAVXXg5I6q+hrw08HiJAHeCtzfNn0GuG5BI5AkLYougXAR8ORAe65tG7QB2JDk4STbk2wac85XA09V1bFTnBOAJLckmU0ye/DgwQ7dlSQtRJdAyIhtNdSeAmaAK4AbgbuTXHCG5+xvrLqrqnpV1Zuenu7QXUnSQnQJhDlg/UB7HbB/RM2WqjpaVU8Ae+gHxHx+BFyQZOoU55QkLaMugbADmGl3Ba0BbgC2DtU8AFwJkGQt/SWkffOdsKoKeBB4Z9t0E7Dl9LouSVpMYwOhrfPfCmwDvgN8oap2JbkzyTWtbBtwKMlu+i/0t1XVIYAkDwFfBN6WZC7Jb7RjPgJ8OMle+u8pfHIxByZJOj3p/7K+OvR6vZqdnV3pbkjSqpJkZ1X1xtX5SWVJEmAgSJIaA0GSBBgIkqTGQJAkAQaCJKkxECRJgIEgSWoMBEkSYCBIkhoDQZIEGAiSpMZAkCQBBoIkqTEQJEmAgSBJagwESRJgIEiSGgNBkgQYCJKkxkCQJAEGgiSp6RQISTYl2ZNkb5Lb56m5PsnuJLuS3Duw/aYkj7fHTQPbv97O+Uh7XHjmw5EkLdTUuIIkk8Bm4CpgDtiRZGtV7R6omQHuAC6vqsMnX9yTvAr4XaAHFLCzHXu4Hfquqppd1BFJkhakywxhI7C3qvZV1RHgPuDaoZqbgc0nX+ir6kDb/hvAX1bVj9u+vwQ2LU7XJUmLqUsgXAQ8OdCea9sGbQA2JHk4yfYkmzoe+6m2XPQfkuQ0+y5JWkRdAmHUC3UNtaeAGeAK4Ebg7iQXjDn2XVX1y8Cb2+M9Iy+e3JJkNsnswYMHO3RXkrQQXQJhDlg/0F4H7B9Rs6WqjlbVE8Ae+gEx77FV9YP29afAvfSXpl6gqu6qql5V9aanpzt0V5K0EF0CYQcwk+TSJGuAG4CtQzUPAFcCJFlLfwlpH7ANuDrJK5O8Erga2JZkqtWR5DzgN4HHFmNAkqSFGXuXUVUdS3Ir/Rf3SeCeqtqV5E5gtqq28twL/27gOHBbVR0CSPL79EMF4M6q+nGSn6MfDOe1c/4V8InFHpwkqbtUDb8d8OLV6/Vqdta7VCXpdCTZWVW9cXV+UlmSBBgIkqTGQJAkAQaCJKkxECRJgIEgSWoMBEkSYCBIkhoDQZIEGAiSpMZAkCQBHf5xu7PCV26H//vtle6FJC3MP/hlePt/WvLLOEOQJAHnygxhGZJVklY7ZwiSJMBAkCQ1BoIkCTAQJEmNgSBJAgwESVJjIEiSAANBktSkqla6D50lOQj87QIPXwv8aBG7sxo45nODYz77nel4X1dV0+OKVlUgnIkks1XVW+l+LCfHfG5wzGe/5RqvS0aSJMBAkCQ151Ig3LXSHVgBjvnc4JjPfssy3nPmPQRJ0qmdSzMESdIpnHWBkGRTkj1J9ia5fcT+85N8vu3/ZpJLlr+Xi6vDmD+cZHeSR5N8LcnrVqKfi2ncmAfq3pmkkqzqO1K6jDfJ9e37vCvJvcvdx8XW4ef64iQPJvmb9rP9jpXo52JKck+SA0kem2d/kvxx+zN5NMmvLmoHquqseQCTwPeAfwisAb4FXDZU86+AP2nPbwA+v9L9XoYxXwm8tD3/0Lkw5lb3cuCvge1Ab6X7vcTf4xngb4BXtvaFK93vZRjzXcCH2vPLgO+vdL8XYdxvAX4VeGye/e8AvgIEeBPwzcW8/tk2Q9gI7K2qfVV1BLgPuHao5lrgM+35/cDbkmQZ+7jYxo65qh6sqqdbczuwbpn7uNi6fJ8Bfh/4z8DPlrNzS6DLeG8GNlfVYYCqOrDMfVxsXcZcwCva858H9i9j/5ZEVf018ONTlFwLfLb6tgMXJHnNYl3/bAuEi4AnB9pzbdvImqo6BvwEePWy9G5pdBnzoA/Q/w1jNRs75iT/GFhfVV9azo4tkS7f4w3AhiQPJ9meZNOy9W5pdBnz7wHvTjIH/AXwr5enayvqdP++n5az7f9UHvWb/vBtVF1qVpPO40nybqAH/PMl7dHSO+WYk0wAHwfet1wdWmJdvsdT9JeNrqA/A3woyRuq6qkl7ttS6TLmG4FPV9UfJflnwOfamE8sffdWzJK+fp1tM4Q5YP1Aex0vnEY+W5Nkiv5U81RTtBe7LmMmyb8A/j1wTVU9s0x9Wyrjxvxy4A3A15N8n/5a69ZV/MZy15/rLVV1tKqeAPbQD4jVqsuYPwB8AaCqvgH8Pfr/5s/ZrNPf94U62wJhBzCT5NIka+i/abx1qGYrcFN7/k7gf1V7t2aVGjvmtnzyp/TDYLWvLcOYMVfVT6pqbVVdUlWX0H/f5Jqqml2Z7p6xLj/XD9C/eYAka+kvIe1b1l4uri5j/jvgbQBJfpF+IBxc1l4uv63Ae9vdRm8CflJVP1ysk59VS0ZVdSzJrcA2+ncp3FNVu5LcCcxW1Vbgk/SnlnvpzwxuWLken7mOY/4Y8DLgi+3987+rqmtWrNNnqOOYzxodx7sNuDrJbuA4cFtVHVq5Xp+ZjmP+t8Ankvwb+ssm71vlv9yR5L/RX/Zb294b+V3gPICq+hP675W8A9gLPA28f1Gvv8r//CRJi+RsWzKSJC2QgSBJAgwESVJjIEiSAANBktQYCJIkwECQJDUGgiQJgP8PJ1XjNPWc1MgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.arange(len(train_loss_timeline)), train_loss_timeline, label=\"train\")\n",
    "ax.plot(np.arange(len(dev_loss_timeline)), dev_loss_timeline, label=\"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_sampler = SequentialSampler(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     B-claim       0.00      0.00      0.00      1428\n",
      "     I-claim       0.59      0.66      0.62     16140\n",
      "           O       0.81      0.79      0.80     31331\n",
      "\n",
      "   micro avg       0.72      0.72      0.72     48899\n",
      "   macro avg       0.46      0.48      0.47     48899\n",
      "weighted avg       0.71      0.72      0.72     48899\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "label_map = {i: label for i, label in enumerate(possible_labels)}\n",
    "\n",
    "out_label_list = [[] for _ in range(out_label_ids.shape[0])]\n",
    "preds_list = [[] for _ in range(out_label_ids.shape[0])]\n",
    "\n",
    "for i in range(out_label_ids.shape[0]):\n",
    "    for j in range(out_label_ids.shape[1]):\n",
    "        if out_label_ids[i, j] != pad_token_label_id:\n",
    "            out_label_list[i].append(label_map[out_label_ids[i][j]])\n",
    "            preds_list[i].append(label_map[preds[i][j]])\n",
    "\n",
    "results = {\n",
    "    \"loss\": eval_loss,\n",
    "    \"precision\": precision_score(out_label_list, preds_list),\n",
    "    \"recall\": recall_score(out_label_list, preds_list),\n",
    "    \"f1\": f1_score([o for sentence in out_label_list for o in sentence], [p for sentence in preds_list for p in sentence])\n",
    "}\n",
    "\n",
    "trues = [o for sentence in out_label_list for o in sentence]\n",
    "predicteds = [p for sentence in preds_list for p in sentence]\n",
    "\n",
    "print(classification_report(trues, predicteds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "15440",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-c72315a558ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mout_label_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mpad_token_label_id\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0mout_label_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mout_label_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                 \u001b[0mpreds_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     results = {\n",
      "\u001b[0;31mKeyError\u001b[0m: 15440"
     ]
    }
   ],
   "source": [
    "with torch.cuda.device(1):\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    preds = np.argmax(preds, axis=2)\n",
    "\n",
    "    label_map = {i: label for i, label in enumerate(labels_test)}\n",
    "\n",
    "    out_label_list = [[] for _ in range(out_label_ids.shape[0])]\n",
    "    preds_list = [[] for _ in range(out_label_ids.shape[0])]\n",
    "\n",
    "    for i in range(out_label_ids.shape[0]):\n",
    "        for j in range(out_label_ids.shape[1]):\n",
    "            if out_label_ids[i, j] != pad_token_label_id:\n",
    "                out_label_list[i].append(label_map[out_label_ids[i][j]])\n",
    "                preds_list[i].append(label_map[preds[i][j]])\n",
    "\n",
    "    results = {\n",
    "        \"loss\": eval_loss,\n",
    "        \"precision\": precision_score(out_label_list, preds_list),\n",
    "        \"recall\": recall_score(out_label_list, preds_list),\n",
    "        \"f1\": f1_score(out_label_list, preds_list)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "tensor(-100, device='cuda:1')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-f1697f833e91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlabel_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid_tags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: tensor(-100, device='cuda:1')"
     ]
    }
   ],
   "source": [
    "label_map[valid_tags[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
