{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification results\n",
    "\n",
    "This notebook contains functions to measure the performance of a classifier applied over a set of annotations.\n",
    "\n",
    "The output of the classification files is expected to be compatible with the output of the UKPLab models. Each line contains the token, the true label, and predicted label of one classification example, in that order, separated by tabs. The first row has the columns names. For example:\n",
    "\n",
    "`word_n    I-Premise:2:Support    I-Premise:-1:Support`\n",
    "\n",
    "Training results must be stored in tsv format with the following columns: epoch, modelName, dev_score, test_score, max_dev_score, max_test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "sns.set_style('white')\n",
    "sns.set_palette('colorblind')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CC_EXPERIMENTS_DIRS = [\n",
    "    ## Full partitions\n",
    "#     '../../results/ukpnets/paragraph/18-08-17-20-19/',  # Time pre-attention no act\n",
    "#    '../../results/ukpnets/paragraph/18-08-18-14-19/',  # Baseline, same as 18-06-07-09-40   !!!!\n",
    "#     '../../results/ukpnets/paragraph/18-08-24-15-47/',  # Feature wise unknown activation\n",
    "    ## Exploration 1 / Timewise sigmoid\n",
    "#     '../../results/ukpnets/paragraph/18-08-29-23-12/',\n",
    "#     '../../results/ukpnets/paragraph/18-08-30-01-28/',\n",
    "    ##\n",
    "    ## Exploration 2 / Timewise sigmoid\n",
    "#     '../../results/ukpnets/paragraph/18-08-30-02-46/',  # Best result\n",
    "#     '../../results/ukpnets/paragraph/18-08-30-04-10/',\n",
    "#     '../../results/ukpnets/paragraph/18-08-30-05-00/',\n",
    "#     '../../results/ukpnets/paragraph/18-08-30-06-05/',\n",
    "#     '../../results/ukpnets/paragraph/18-08-30-07-10/',\n",
    "    '/home/mteruel/am/results/ukpnets/paragraph/18-08-30-18-51/', # Full eval timewise sigmoid\n",
    "    ## Exploration 2 / Featurewise sigmoid\n",
    "#     '../../results/ukpnets/paragraph/18-08-31-00-48/',\n",
    "#     '../../results/ukpnets/paragraph/18-08-31-02-46/',\n",
    "#     '../../results/ukpnets/paragraph/18-08-31-05-55/',\n",
    "#     '../../results/ukpnets/paragraph/18-08-31-08-06/',\n",
    "#     '../../results/ukpnets/paragraph/18-08-31-10-26/',\n",
    "    ##\n",
    "    '/home/mteruel/am/results/ukpnets/paragraph/18-08-31-14-58/',  # Full eval featurewise sigmoid\n",
    "    ##\n",
    "#    '../../results/ukpnets/paragraph/18-09-01-14-03/',\n",
    " #   '../../results/ukpnets/paragraph/18-09-01-16-40/',\n",
    "#    '../../results/ukpnets/paragraph/18-09-01-18-46/',\n",
    "#    '../../results/ukpnets/paragraph/18-09-01-19-56/',\n",
    "#    '../../results/ukpnets/paragraph/18-09-01-22-00/',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Results - Dev and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_single_file(filename):\n",
    "    result = pandas.read_csv(filename, sep='\\t')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_filenames(dirname):\n",
    "    return [run_prediction for run_prediction in os.listdir(dirname)\n",
    "            if os.path.isfile(os.path.join(dirname, run_prediction)) and 'predictions' in run_prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_single_classifier(classifier_dirpath, dataset_name='ukp'):\n",
    "    \"\"\"Reads all partitions for a single classifier.\"\"\"\n",
    "    result = pandas.DataFrame(columns=[\n",
    "        'Classifier', 'Partition', 'Dataset',\n",
    "        'Accuracy', 'Precision', 'Recall', 'F1-Score', 'Support'])\n",
    "    for index, prediction_file in enumerate(prediction_filenames(classifier_dirpath)):\n",
    "        predictions = labels_single_file(os.path.join(classifier_dirpath, prediction_file))\n",
    "        accuracy = metrics.accuracy_score(predictions['True'], predictions.Predicted)\n",
    "        precision, recall, f1, _ = metrics.precision_recall_fscore_support(\n",
    "            predictions['True'], predictions.Predicted, average='weighted', warn_for=[])\n",
    "        support = len(predictions)\n",
    "        if 'partition' in prediction_file: \n",
    "            partition = re.search('.*partition(\\d+).*', prediction_file).group(1)\n",
    "        else:\n",
    "            partition = 0\n",
    "        dataset = re.search('.*{}_(\\w+).conll'.format(dataset_name), prediction_file).group(1)\n",
    "        classifier = os.path.basename(os.path.normpath(classifier_dirpath))\n",
    "        result.loc[index] = [classifier, partition, dataset, accuracy, precision, recall, f1, support]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiments_metrics(experiments_dirs, dataset_name='ukp'):\n",
    "    \"\"\"Reads metrics for all classifiers\"\"\"\n",
    "    classifier_metrics = []\n",
    "    for classifier_path in experiments_dirs:\n",
    "        classifier_metrics.append(metrics_single_classifier(classifier_path, dataset_name))\n",
    "    return pandas.concat(classifier_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRIC_COLS = ['Accuracy', 'Precision', 'Recall', 'F1-Score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMPONENT CLASSIFICATION\n",
    "\n",
    "### Classifier summary`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_single_file(filename):\n",
    "    result = pandas.read_csv(\n",
    "        filename, sep='\\t', header=None,\n",
    "        names=['epoch', 'model_name', 'dev_score', 'test_score', 'max_dev_score', 'max_test_score'])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_filenames(dirname):\n",
    "    return [learning_metrics for learning_metrics in os.listdir(dirname)\n",
    "            if os.path.isfile(os.path.join(dirname, learning_metrics)) and 'results' in learning_metrics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_metrics_single_classifier(classifier_dirpath):\n",
    "    \"\"\"Reads all partitions for a single classifier.\"\"\"\n",
    "    result = []\n",
    "    for index, results_file in enumerate(learning_filenames(classifier_dirpath)):\n",
    "        learning_metrics = learning_single_file(os.path.join(classifier_dirpath, results_file)).drop(\n",
    "            columns=['model_name', 'max_dev_score', 'max_test_score'])\n",
    "        learning_metrics = learning_metrics.set_index(['epoch']).stack().reset_index().rename(\n",
    "            columns={0: 'Metric Value', 'level_1': 'Dataset'})\n",
    "        learning_metrics['Partition'] = re.search('.*partition(\\d+).*', results_file).group(1)\n",
    "        learning_metrics['Classifier'] = os.path.basename(os.path.normpath(classifier_dirpath))\n",
    "        result.append(learning_metrics)\n",
    "    return pandas.concat(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_metrics(experiment_dirs):\n",
    "    \"\"\"Reads learning metrics for all classifiers\"\"\"\n",
    "    classifier_learning_metrics = []\n",
    "    for classifier_path in experiment_dirs:\n",
    "        classifier_learning_metrics.append(learning_metrics_single_classifier(classifier_path))\n",
    "    return pandas.concat(classifier_learning_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ESSAYS_EXPERIMENTS_DIRS = [\n",
    "    ## Exploration 2 / Timewise sigmoid\n",
    "'/home/mteruel/am/results/essays2/18-11-11-13-00',\n",
    "'/home/mteruel/am/results/essays2/18-11-11-13-09',\n",
    "'/home/mteruel/am/results/essays2/18-11-11-13-20',\n",
    "'/home/mteruel/am/results/essays2/18-11-11-13-30',\n",
    "'/home/mteruel/am/results/essays2/18-11-11-13-45',\n",
    "'/home/mteruel/am/results/essays2/18-11-11-13-55',\n",
    "'/home/mteruel/am/results/essays2/18-11-11-14-07',\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Classifier</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18-11-11-13-00</th>\n",
       "      <td>0.670899</td>\n",
       "      <td>0.665013</td>\n",
       "      <td>0.670899</td>\n",
       "      <td>0.663706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18-11-11-13-09</th>\n",
       "      <td>0.687827</td>\n",
       "      <td>0.685012</td>\n",
       "      <td>0.687827</td>\n",
       "      <td>0.677986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18-11-11-13-20</th>\n",
       "      <td>0.653224</td>\n",
       "      <td>0.651541</td>\n",
       "      <td>0.653224</td>\n",
       "      <td>0.650573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18-11-11-13-30</th>\n",
       "      <td>0.684093</td>\n",
       "      <td>0.682142</td>\n",
       "      <td>0.684093</td>\n",
       "      <td>0.680774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18-11-11-13-45</th>\n",
       "      <td>0.664426</td>\n",
       "      <td>0.656226</td>\n",
       "      <td>0.664426</td>\n",
       "      <td>0.657778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18-11-11-13-55</th>\n",
       "      <td>0.681603</td>\n",
       "      <td>0.671461</td>\n",
       "      <td>0.681603</td>\n",
       "      <td>0.669277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18-11-11-14-07</th>\n",
       "      <td>0.697038</td>\n",
       "      <td>0.687080</td>\n",
       "      <td>0.697038</td>\n",
       "      <td>0.682246</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Accuracy  Precision    Recall  F1-Score\n",
       "Classifier                                             \n",
       "18-11-11-13-00  0.670899   0.665013  0.670899  0.663706\n",
       "18-11-11-13-09  0.687827   0.685012  0.687827  0.677986\n",
       "18-11-11-13-20  0.653224   0.651541  0.653224  0.650573\n",
       "18-11-11-13-30  0.684093   0.682142  0.684093  0.680774\n",
       "18-11-11-13-45  0.664426   0.656226  0.664426  0.657778\n",
       "18-11-11-13-55  0.681603   0.671461  0.681603  0.669277\n",
       "18-11-11-14-07  0.697038   0.687080  0.697038  0.682246"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_results = experiments_metrics(ESSAYS_EXPERIMENTS_DIRS, 'essays').set_index('Classifier')\n",
    "es_results[es_results.Dataset == 'dev'][METRIC_COLS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_confusion_matrix(confusion_matrix, class_names, figsize = (10,7), fontsize=14):\n",
    "    \"\"\"Prints a confusion matrix, as returned by sklearn.metrics.confusion_matrix, as a heatmap.\n",
    "    \n",
    "    Args:\n",
    "        confusion_matrix: numpy.ndarray\n",
    "            The numpy.ndarray object returned from a call to sklearn.metrics.confusion_matrix. \n",
    "            Similarly constructed ndarrays can also be used.\n",
    "        class_names: list\n",
    "            An ordered list of class names, in the order they index the given confusion matrix.\n",
    "        figsize: tuple\n",
    "            A 2-long tuple, the first value determining the horizontal size of the ouputted figure,\n",
    "            the second determining the vertical size. Defaults to (10,7).\n",
    "        fontsize: int\n",
    "            Font size for axes labels. Defaults to 14.\n",
    "        \n",
    "    Returns:\n",
    "        The resulting confusion matrix figure\n",
    "    \"\"\"\n",
    "    df_cm = pandas.DataFrame(\n",
    "        confusion_matrix, index=class_names, columns=class_names, \n",
    "    )\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    try:\n",
    "        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Confusion matrix values must be integers.\")\n",
    "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n",
    "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(classifier_name, partition=0):\n",
    "    for classifier_dirname in CC_EXPERIMENTS_DIRS + REL_EXPERIMENTS_DIRS:\n",
    "        if not classifier_name in classifier_dirname:\n",
    "            continue\n",
    "        prediction_files = prediction_filenames(classifier_dirname)\n",
    "        prediction_file = None\n",
    "        for possible_prediction_file in prediction_files:\n",
    "            if 'partition' + str(partition) in possible_prediction_file:\n",
    "                prediction_file = possible_prediction_file\n",
    "                break\n",
    "        if prediction_file is None:\n",
    "            raise ValueError('No prediction for the given partition')\n",
    "        predictions = labels_single_file(os.path.join(classifier_dirname, prediction_file))\n",
    "        labels = numpy.unique(numpy.concatenate([predictions['True'].values, predictions.Predicted.values]))\n",
    "        print(metrics.classification_report(predictions['True'], predictions.Predicted, labels=labels))\n",
    "        cm = metrics.confusion_matrix(predictions['True'], predictions.Predicted, labels=labels)\n",
    "        print(cm.sum())\n",
    "        print_confusion_matrix(cm, labels)\n",
    "        return None\n",
    "    raise ValueError('Classifier not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'REL_EXPERIMENTS_DIRS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-131a13dd005f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_confusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'18-06-07-09-40'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-e1faf76ea53a>\u001b[0m in \u001b[0;36mplot_confusion_matrix\u001b[0;34m(classifier_name, partition)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_confusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mclassifier_dirname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCC_EXPERIMENTS_DIRS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mREL_EXPERIMENTS_DIRS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclassifier_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclassifier_dirname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprediction_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprediction_filenames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier_dirname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'REL_EXPERIMENTS_DIRS' is not defined"
     ]
    }
   ],
   "source": [
    "plot_confusion_matrix('18-06-07-09-40', partition=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'REL_EXPERIMENTS_DIRS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-349db9e12321>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_confusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'18-06-01-06-57'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-e1faf76ea53a>\u001b[0m in \u001b[0;36mplot_confusion_matrix\u001b[0;34m(classifier_name, partition)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_confusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mclassifier_dirname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCC_EXPERIMENTS_DIRS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mREL_EXPERIMENTS_DIRS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclassifier_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclassifier_dirname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprediction_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprediction_filenames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier_dirname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'REL_EXPERIMENTS_DIRS' is not defined"
     ]
    }
   ],
   "source": [
    "plot_confusion_matrix('18-06-01-06-57', partition=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:am]",
   "language": "python",
   "name": "conda-env-am-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
